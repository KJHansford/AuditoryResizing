---
title: Multisensory Integration and Proprioceptive Drift During Resizing Illusions
output:
  bookdown::pdf_document2:
    fig_caption: yes
    toc: no
    keep_tex: no
  pdf_document: default
  word_document: default
header-includes:
- \usepackage{indentfirst}
- \usepackage{float}
- \usepackage{float} \floatplacement{figure}{H}
- \setcounter{figure}{0}
- \newcommand{\beginsupplement}{\setcounter{table}{1}  
- \renewcommand{\thetable}{S\arabic{table}} \setcounter{figure}{1} 
- \renewcommand{\thefigure}{S\arabic{figure}}}
- \usepackage{caption}
indent: yes
bibliography: references.bib
link-citations: true
linkcolor: black
csl: elife.csl
---

\vspace{10mm}
\begin{center}
Kirralise J. Hansford$^{1}$, Kirsten J. McKenzie$^{2}$, Daniel H. Baker$^{1}$, and Catherine Preston$^{1}$
\vspace{30mm}

$^{1}$ Department of Psychology, University of York
\vspace{5mm}

$^{2}$ School of Psychology, University of Lincoln
\vspace{50mm}

\end{center}

\pagebreak

# Abstract {.unnumbered}

Bodily resizing illusions typically use visual and/or tactile inputs to produce a vivid experience of one’s body changing size. Naturalistic auditory input (input that reflects the natural sounds of a stimulus) has been used to increase illusory experience during the rubber hand illusion, whilst non-naturalistic auditory input can influence estimations of finger length. We aimed to use non-naturalistic auditory input during a hand-based resizing illusion using augmented reality, to assess whether the addition of auditory input would increase both subjective illusion strength and measures of performance-based tasks. 44 participants completed three conditions: no finger stretching, finger stretching without tactile feedback, and finger stretching with tactile feedback. Half the participants had auditory input throughout all conditions, while the other half did not. After each condition, participants were given one of three performance tasks: stimulated (right) hand dot touch task, non-stimulated (left) hand dot touch task and a ruler judgement task. Dot tasks involved participants reaching for the location of a virtual dot, whereas the ruler task concerned estimates of the participant’s own finger on a ruler whilst the hand was hidden from view. After all trials, participants completed a questionnaire capturing subjective illusion strength. The addition of auditory input increased subjective illusion strength for manipulations without tactile feedback but not those with tactile feedback. No facilitatory effects of audio were found for any performance task. We, therefore, conclude that adding auditory input to illusory finger stretching increased subjective illusory experience in the absence of tactile feedback but did not affect performance-based measures.


\vspace{5mm}
*Keywords*: Multisensory Integration, Resizing Illusions, Proprioceptive Drift, Audition. 


```{r setup, include=FALSE}

processdata <- 0        # takes about 6.5 hours to run from scratch

packagelist <- c('R.matlab','ggpubr','tidyverse','ez','lmerTest','ggplot2','MuMIn', 'emmeans','kableExtra','gridExtra','pwr','reshape2','rstatix','osfr','jpeg','grid')
missingpackages <- packagelist[!packagelist %in% installed.packages()[,1]]
if (length(missingpackages)>0){install.packages(missingpackages)}
toinstall <- packagelist[which(!packagelist %in% (.packages()))]
invisible(lapply(toinstall,library,character.only=TRUE))

knitr::opts_chunk$set(echo = TRUE)

threshold <- 0.7   # this is the threshold for the image analysis
outputimages <- 0  # outputs finger images for checking

if (!dir.exists('local')){dir.create('local')}

if (!file.exists('local/Final Dataset(r)_Outliers Removed.csv')){
  osfproject <- osf_retrieve_node("b9s48")
  osffiles <- osf_ls_files(osfproject)
  fid <- which(osffiles$name=='Final Dataset(r)_Outliers Removed.csv')
  osf_download(osffiles[fid,], path='local/',progress=TRUE)
}

#load in the subjective data
subjective_data <- read.csv("local/Final Dataset(r)_Outliers Removed.csv")

includedsubjects <- subjective_data[seq(1,132,3),1]
groupassignments <- factor(subjective_data[seq(1,132,3),4])
audiogroup <- which(groupassignments=='Audio')
nonaudiogroup <- which(groupassignments=='No Audio')

```

## 1. Introduction {.unnumbered}

Resizing illusions can be delivered through either augmented reality or magnifying optics and typically use combined visual and tactile inputs to manipulate the size of a body part, making it appear either larger or smaller. These illusions, through changing the way a body part is perceived, exploit principles of multisensory integration to elicit modulations in the perceived size and shape of the body part [@preston2011a; @preston2020a; @stanton2018a]. In addition to visual and tactile illusions, the combination of visual and proprioceptive, or, visual and motor inputs, has also been found to elicit body resizing illusions, with research demonstrating that proprioceptively aligning a child avatar body with a participant’s adult body can elicit a strong illusion of having a smaller child-sized body [@banakou2013a] and further research showing similarly that synchronous movements of an avatar with an elongated arm influences participants’ judgements of arm length [@kilteni2012a]. Furthermore, tasks using combined visuotactile inputs have been compared to those employing unimodal visual inputs for finger stretching illusions, with participants reporting greater subjective embodiment of the illusion during the combined visuotactile stimulation than during the unimodal visual illusions [@hansford2023a]. Such findings serve to highlight the importance of multisensory integration for subjective embodiment during illusory changes in finger length. 

Multisensory processing helps us to perceive a stimulus as a single coherent experience, despite being comprised of a combination of several different sensory inputs. This process is thought to be important for experiencing our body as our own, as has been demonstrated during the rubber hand illusion, whereby the simultaneous visual and tactile stimulation of a rubber hand, at the same time and location as inputs applied to a participant’s own visually-occluded hand, can manipulate our understanding of what we experience to be part of our own body [@botvinick1998a]. Many of the theories explaining body ownership and multisensory body illusions focus primarily on tactile and proprioceptive inputs [@tsakiris2010a; @botvinick1998a] as these senses are thought to be unique to bodily experience, in contrast to sensory inputs such as vision and audition which are experienced both in relation to our own body and to objects in the external world. However, more recent Bayesian accounts of body ownership suggest that the addition of other senses may also facilitate feelings of embodiment and vividness of body illusions [@kilteni2015a]. Studies have claimed additive effects of additional senses in multisensory integration concerning non-body events, finding that the addition of auditory stimuli enhanced overall efficiency in difficult visual detection tasks [@frassinetti2002a]. This has also been demonstrated as an inverse effect, wherein the addition of visual cues to an auditory task can improve the detection of a low-intensity sound [@lovelace2003a]. Additionally, above threshold, there is evidence supporting the modulation of tactile perception via audio cues; a study by @zampini2004a found that increasing the overall volume and / or the amplitude of high frequency sounds, combined with the tactile input of biting a potato chip, increased the reported crispness of the chip. 

Research examining multisensory integration relating to the body and body illusions has also begun to explore the importance of other senses; notably, the role of auditory input in multisensory interactions, which have been found to influence perceptions of body size and length [@tajadura-jim2012a], as well as altering perceived material properties [@senna2014a] and the weight [@tajadura-jim2015a] of the body. Looking specifically at visual, tactile, and auditory inputs within the rubber hand illusion (which is used as a theoretical basis for the embodiment experienced in resizing illusions), @omera2014a used proprioceptive drift tasks, which measure localisation bias after proprioceptive manipulations, to demonstrate that adding auditory inputs consistent with the visual and tactile inputs related to stroking the hand (in this instance the sound of sandpaper scratching the skin) heightened the illusory experience more than when white noise was added to the illusion. This is further supported by the findings of @radziun2018a, who also looked at the addition of ecologically relevant auditory inputs to the rubber hand illusion. Their study used the sound of a surface being stroked with a paintbrush, subjective questionnaires, and proprioceptive drift tasks to demonstrate that synchronous auditory cues made the illusion stronger, compared to asynchronous auditory cues. 

The addition of auditory input in the studies mentioned above involved naturalistic auditory input, i.e., experimental auditory input that was consistent with realistic auditory stimuli, such as we are used to encountering in everyday life. However, @tajadura-jim2017a, looked at the influence of non-naturalistic auditory inputs, to see if this still resulted in changes to body perception. Here, they used changes in pitch, due to their associations with a change in height or size [@hubbard2018a], and which are not typically associated with bodily movement. They found that when participants pulled their own right index finger with their left hand, with an accompanying rising pitch sound (700 – 1200Hz), they estimated the length of their index finger to be longer than when this pulling was accompanied with either a descending (700 – 200Hz) or constant (700Hz) tone and coined this the “Auditory Pinocchio” effect (although they did not attempt to stretch participants’ noses).  

Given these previous findings involving naturalistic auditory input in the rubber hand illusion [@omera2014a] and non-naturalistic auditory input in auditory-tactile resizing manipulations [@tajadura-jim2017a], it is plausible that the addition of non-naturalistic auditory input, when using augmented reality to induce visual and visual-tactile resizing illusions, could heighten the strength of the illusory experience. This prediction refers again to the notion that the inclusion of more senses provides a more holistic and vivid experience of an event [@kilteni2015a].

Measuring the experience of illusory effects often consists of questionnaires given to participants after they have experienced an illusory condition to gain a subjective measure of their experience. However, more performance-based evidence can also be taken from behavioural measures of proprioceptive drift, which is defined as the change in proprioceptively perceived position of the participants hidden body part [davies2013a]. Previous studies assessing proprioceptive drift during the rubber hand illusion have found differences regarding the influence of the illusion on body schema. Body schema are representations of the body based on bottom-up sensory inputs that are needed for action, and are thought to be distinct from body image, which refers to a top-down body representation that is needed for perception [@paillard1999a]. @kammers2009a investigated the relationship between body schema and body image using the rubber hand illusion with a reaching proprioceptive drift task (action task), wherein participants were asked to reach with one hand to point to the tip of the index finger of the other hand in a single movement, to assess body schema. Participants were also asked to verbally report when the experimenter’s moving finger represented the felt location of their own finger (perceptual task), to assess body image. @kammers2009a, found that only the perceptual judgements regarding limb ownership were sensitive to distortion in the rubber hand illusion, concluding that action movements, and therefore body schema, were not affected. In contrast, @newport2010a used augmented reality and a dot touch proprioceptive drift task with supernumerary limbs to assess body schema using a virtual version of the rubber hand illusion, and found that distortions in body schema were apparent, evidenced through pointing errors in the dot touch task that were consistent with the remapped limb position. 

A point to note within this previous research is that the terms “subjective” and “performance task” can be used to refer to several concepts in relation to data regarding bodily experience. For the purposes of the current study, the term “subjective” is used to refer to data collected from self-report questionnaires, whereas the term “performance task” is taken to refer to data collected from proprioceptive drift and ruler judgement tasks, such as those used by @davies2013a, @kammers2009a and @newport2010a. This is due to self-report tasks indexing personal, subjective, experience of resizing illusions, whereas proprioceptive drift and ruler judgement tasks index more impartial, performance based, data regarding the effects of resizing illusions on one’s percept of their bodily experience. 

Given previous research showing the additive effects of including several different sensory inputs for overall experience, and the recent evidence of the effects that additional auditory inputs can have on illusory experience when compared to unimodal stimulation alone, we hypothesise that adding a non-naturalistic auditory input (one that is consistent with the visual and tactile manipulations of stretching a finger) during augmented-reality resizing illusions, will (1) heighten illusion strength, measured via a subjective illusory experience questionnaire, for (1a) visual and (1b) visuo-tactile manipulations. In addition, we hypothesise (2) that the addition of auditory input will lead to stronger illusions as indexed by performance tasks, in line with experience of a longer finger, as measured using a dot touch proprioceptive drift task that indexes body schema for (2a) visual and (2b) visuo-tactile manipulations. We also hypothesise that the addition of auditory input will increase judgements of finger length, measured using a ruler judgement task that indexes body image for (3a) visual and (3b) visuo-tactile manipulations. Our inclusion of two different proprioceptive drift tasks; a dot touch task and a ruler judgement task, aims to address the apparent discordance between the findings of @kammers2009a and @newport2010a, relating to the effects of resizing illusions upon body image and body schema. 


## 2. Method {.unnumbered}

## 2.1 Pre-registration {.unnumbered}
Pre-registration of this study can be found at the following OSF link: https://osf.io/6x4ce 

## 2.2 Ethical Approval {.unnumbered}
This study was granted ethical approval from the University of York and was performed in accordance with the ethical standards as laid down in the 1964 Declaration of Helsinki.

## 2.3 Participant Sample {.unnumbered}

```{r poweranalyses, include=FALSE}

#subjective analysis power analysis
SubjectivePA <- pwr.t.test(d=1.02,power=0.8,sig.level=0.05,type="two.sample",alternative="greater")
#objecitve dot touch power analysis
ObjectivePA <- pwr.anova.test(k=2,f=.64,sig.level=.05,power=.8)

```

## 2.3.1 Power Analysis and Sample Size {.unnumbered}

A priori power analysis using subjective illusion data and performance task dot touch data from a pilot study (N = 10, https://osf.io/pb3ku ) showed a minimum sample size of `r round(SubjectivePA$n[1])*2` participants is required for hypothesis 1a regarding visuo-auditory / visuotactile-auditory manipulations (d = `r (SubjectivePA$d[1])`, power = `r (SubjectivePA$power[1])`, alpha = `r (SubjectivePA$sig.level[1])`), and a sample of `r round(ObjectivePA$n[1])*2` participants is required for hypothesis 2 regarding the dot touch task (f = `r (ObjectivePA$f[1])`, power = `r (ObjectivePA$power[1])`, alpha = `r (ObjectivePA$sig.level[1])`). Due to the inherent ambiguity of power analysis, and to account for the additional ruler judgment task, the upper sample size of 26 participants will be doubled to a sample size of 52 participants.

## 2.3.2 Participants {.unnumbered}

52 participants (84.6% Female. 11.5% Male, 3.8% Non-Binary; Mean age = 19.3 years, age range = 18 – 24 years) gave informed consent and completed the experiment. Exclusion criteria were detailed on the participant information sheet and included: prior knowledge or expectations about the research, a history of neurological or psychiatric disorders, any operations or procedures that could damage peripheral nerve pathways in the hands, a history of chronic pain conditions, history of drug or alcohol abuse, history of sleep disorders, history of epilepsy, having visual abnormalities that cannot be corrected optically (i.e. with glasses), or being under 18 years of age. From these 52 participants, 8 scored above 50 (indicating experience of the illusion) on the subjective experience questionnaire item regarding feeling stretching of the finger within the baseline condition where no stretching took place, therefore these 8 participants were removed from subsequent analyses, resulting in `r n_distinct(subjective_data$PID)` participants being included in the final sample; 23 in the No-Audio group, and 21 in the Audio group. 

## 2.4 Materials {.unnumbered}

The resizing illusions were delivered using an augmented-reality system (see Figure 1) that consisted of an area for the hands to be placed which contained a black felt base, LED lights mounted on either side and a 1920 x 1080 camera situated in the middle of the area, away from the participant’s view. Above this area, there was a mirror placed below a 1920 x 1200 resolution screen, so that the footage from the camera was reflected by the mirror such that the participant could view live footage of their own occluded hands. The manipulation of the live feed from the camera was implemented using MATLAB r2017a, wherein the participant’s finger would stretch by 60 pixels (2.1 cm) during illusions lasting 2.4 seconds. This stretching was accompanied during the visuotactile / visuotactile-auditory conditions by the experimenter gently pulling on the participant’s right index finger to provide tactile input and induce immersive multisensory illusions. In the audio group, the stretching manipulations in the visuotactile-auditory and the visual-auditory conditions were accompanied by a pure tone that increased linearly in frequency from 308Hz – 629Hz, whereas trial in which no stretching took place were accompanied by a 440Hz tone. Auditory input was delivered by two speakers located beneath the augmented reality system. This positioning of the speakers was to ensure that the location of the sound was aligned with the location of the resizing manipulations (based on feedback from the pilot study that suggested auditory input delivered further from the augmented reality system created a disconnect between the different sensory inputs). After each condition, the participants’ hands were occluded from view and the dot touch or ruler judgement tasks were presented (detailed in section 2.3), until the experimenter pressed a button to indicate the start of the next trial. A blue rectangle was superimposed on the screen so that participants knew where to reposition their hands to after each task. Subjective illusion experience data were collected via Qualtrics (Qualtrics, Provo, UT) on a Samsung Galaxy Tab A6 tablet. This was given to participants after all experimental trials were presented, when each manipulation was presented again, without the subsequent tasks, and participants were asked to recall the trial they had just experienced and previous trials that were similar, and then give a response on a visual analogue scale of 0 to 100, with 0 being strongly disagree, 50 being neutral, and 100 being strongly agree, with written statements. The questionnaire consisted of six statements, two relating to illusory experience: “It felt like my finger was really stretching” / “It felt like the hand I saw was part of my body”, two relating to disownership: “It felt like the hand I saw no longer belonged to me” / “It felt like the hand I saw was no longer part of my body”, and two were control statements: “It felt as if my hand had disappeared” / “It felt as if I might have had more than one right hand”. The questionnaire was delivered 3 times, once after baseline manipulations, once after visuotactile / visuotactile-auditory manipulations, and finally once after unimodal visual /  visual-auditory manipulations. 


```{r figure1, fig.cap="Schematic of Augmented Reality System.", fig.align='center', echo=FALSE}

knitr::include_graphics('figures/Augmented Reality Infographic.png')

```

## 2.5 Procedure {.unnumbered}

Participants were assigned to either the auditory group or the non-auditory group based on a randomised MATLAB output of the total number of participants split randomly and evenly into two groups. They were then seated at the augmented-reality system and were instructed to place both of their hands onto the felt lining, with their index fingers outstretched. There were four white dots on the felt to guide where their hands should be placed, creating 2 hand spaces (one between each pair of dots), and arm rests were provided for comfort. Participants were instructed to view the image of their hands in the mirror (whilst their real hands were hidden from view) throughout the experiment. They viewed their hands whilst receiving baseline conditions in which no manipulations were applied (with a 440Hz tone played for auditory group), stretching conditions in which they saw the index finger on their right-hand visually stretch (unimodal visual / visual-auditory conditions with accompanying 308Hz – 629Hz sound for the auditory group), and stretching conditions in which as they saw their index finger on their right hand stretch as a researcher gently pulled on the end of their finger simultaneously (visuotactile / visuotactile-auditory conditions with accompanying 308Hz – 629Hz sound for the auditory group).

After viewing the manipulation of their right hand, participants completed either a left-hand dot touch task, a right-hand dot touch task, or a ruler judgement task. The dot touch tasks consisted of the participant’s hands being occluded from view before a magenta dot appeared in front of either their right or left hand, and participants were then asked to move their index finger in one smooth ballistic pointing movement to touch the dot. When the participant had completed this movement, they were asked to leave their finger in place for a few seconds whilst the experimenter pressed a button to record an image of the hand position through the camera. The participant then returned their hand to the indicated pre-trial position. The ruler judgement task consisted of the participant’s hands again being occluded from view before a 14 cm ruler, with 8 marks spaced 2cm apart, was displayed to the right of the participant’s right hand. The ruler changed in position and scale to avoid trial order bias. The start-point of the scale ranged from 10 to 60 cm’s, and the vertical position of the ruler was jittered using a normal distribution with a mean of 0 and standard deviation of 40 pixels. Participants were asked to verbally indicate the location on the ruler that corresponded with where they felt the tip of their right (stimulated) index finger was. 

Participants completed 6 repetitions of 9 distinct conditions which can be seen in Table 1. A Video of a participant undergoing visuotactile stretching can be seen in supplementary material. Conditions were randomised via MATLAB r2017a, and the experimenter was unaware which condition would be presented on a given trial. The experimenter was informed whether to gently pull the index finger or to apply no manipulation via the presentation of a small blue rectangle on the screen, out of the participant’s view. 6 repetitions of the 9 conditions were presented, followed by a break for the participant to remove their hands from the box and rest, and then the baseline, visuotactile / visuotactile-auditory and the unimodal visual / visual-auditory conditions were presented once in a random order, without any dot touch or ruler judgement tasks, after which the participant completed the subjective illusory experience questionnaire.

```{r table1prep, echo=FALSE, include = FALSE}

##Table 1##

#Set up a blank dataframe with named columns.
col1 <- c('Baseline with Left Dot Touch','V/VA stretching with left dot touch', 'VT/VTA stretching with left dot touch')
col2 <- c('Baseline with right dot touch','V/VA stretching with right dot touch', 'VT/VTA stretching with right dot touch')
col3 <- c('Baseline with ruler judgement task','V/VA stretching with ruler judgement task','VT/VTA stretching with ruler judgment task')
  
# merge three column vectors into a matrix
mat <- cbind(col1, col2, col3)

as.data.frame(mat)

columns <- c('','Conditions','')
colnames(mat) <- columns

```

``` {r table1, echo=FALSE}
#Format and knit dataframe as table.
knitr::kable(mat, booktabs = T, caption='Distinct Conditions with Associated Tasks', align='ccc') %>%
  row_spec(0,bold=T) %>% 
  kable_styling(latex_options= c("scale_down","HOLD_position"), font_size = 9)

```

## 2.6 Analysis {.unnumbered}

During each trial a still image was taken of the location of the participant’s hands within the augmented reality system, to be used for analysis of the dot touch and ruler judgement data. Preprocessing was done algorithmically using image intensity data to estimate finger position; details of this can be seen in the code available on OSF at the following link: https://osf.io/b9s48/. All data and code for analysis are available on the OSF page, which also contains resources to computationally reproduce this manuscript, including all analyses, figures, and statistical outputs, from the raw data. For the dot touch data, the images were used to determine how far away the participant’s finger was from the magenta dot, which was stored as an error rating for each trial and then averaged across the same trial types for each participant. This was completed for both left and right dot touch tasks. The ruler judgement data analysis consisted of using the still images with the superimposed ruler and the ruler ratings given verbally by the participant during the experimental task to check that the rating given was within the range of the ruler. If this was not true, as was the case with 4 participants, then their data for those trials were removed before statistical analysis. Then, the differences between the given ruler ratings and the actual tips of the fingers on the still images were used to generate error values, which were then used for statistical analysis. 

For statistical analysis of the data, linear mixed effects models were used for all hypothesis testing, as these are robust to missing data and unequal groups. 

## 3. Results {.unnumbered}

```{r analyseraw, include=FALSE}

if (processdata==1){
if (!dir.exists('local/rawdata')){dir.create('local/rawdata')}
if (!dir.exists('local/fingerimages')){dir.create('local/fingerimages')}

osfproject <- osf_retrieve_node("b9s48")

osffiles <- osf_ls_files(osfproject)
if (!file.exists('local/rulerratings.csv')){
  fid <- which(osffiles$name=='rulerratings.csv')
  osf_download(osffiles[fid,], path='local/',progress=TRUE)
}
ratings <- read.csv('local/rulerratings.csv',header=FALSE)

osffiles <- osf_ls_files(osfproject,path='Raw Data',n_max=100)
for (n in 1:nrow(osffiles)){
if (!file.exists(paste0('local/rawdata/',osffiles[n,1]))){
  osf_download(osffiles[n,], path='local/rawdata/',progress=TRUE)
}}


sublist <- c(402:440,442:454)

allout <- array(0,dim=c(length(sublist),9,6))

for (s in 1:length(sublist)){
  subjID <- sublist[s]
  
  print(subjID)
  
  temp <- readMat(paste0('local/rawdata/',subjID,'trialorder.mat'))
  
  subjind <- which(ratings[,1]==subjID)
  
  triallist <- unlist(temp$trialorder[[1]])
  
  output <- matrix(0,nrow=9,ncol=6)
  condcount <- (1:9)*0
  rulercount <- 0
  
  for (trial in 1:length(triallist)){
    
    cond <- triallist[trial]
    condcount[cond] <- condcount[cond] + 1
 
    # mirror reverse the image because the camera is pointing at a mirror!
    thisim <- temp$allimages[trial,,1920:1,]
    imwithdot <- thisim
    
    if (triallist[trial]<7){
      if (triallist[trial]<4){print(paste(subjID,'Right dot touch'))}
      if (triallist[trial]>3){print(paste(subjID,'Left dot touch'))}
      
      xy <- round(c(3*1920/4,350) + (c(-1,1)*temp$xydots[trial,]))
      
      # remove the magenta dot
      A <- thisim[,,1]==255
      B <- thisim[,,2]==0
      C <- thisim[,,3]==255
      abc <- A*B*C
      ind <- which(abc>0,arr.ind=TRUE)
      thisim[ind[,1],ind[,2],] <- 0
      
      meanmap <- apply(thisim/255,1:2,mean)
      meanmap[1:100,] <- 0
      ind <- which(meanmap>threshold,arr.ind=TRUE)  # find pixels above the threshold
      fingertip <- min(ind[,1])  # find the y value closest to the top of the image
      
      output[cond,condcount[cond]] <- fingertip - xy[2]   # store error     
      
    }

    if (triallist[trial]>6){
      print(paste(subjID,'Ruler task'))
      
      rulercount <- rulercount + 1
      subjectrating <- ratings[subjind,rulercount+1]
      
      meanmap <- apply(thisim/255,1:2,mean)
      meanmap[1:100,] <- 0
      ind <- which(meanmap>threshold,arr.ind=TRUE)  # find pixels above the threshold
      fingertip <- min(ind[,1])  # find the y value closest to the top of the image
      
      thisscale <- temp$rulerscales[trial]
      thispos <- temp$rulerxy[trial]
      
      rulermin <- (1200/2) + 50 - thispos
      rulermax <- (1200/2) - 350 - thispos
      
      truepos <- thisscale + ((rulermin - fingertip)/50)
      
      output[cond,condcount[cond]] <- subjectrating - truepos    
      
      # adjusts the ratings to flag as an out of range trial
      if (subjectrating>(thisscale + 8)){output[cond,condcount[cond]] <- NA}
      if (subjectrating<thisscale){output[cond,condcount[cond]] <- NA}

    } 
    
          if (outputimages==1){

    imwithdot[fingertip,,1] <- 0
    imwithdot[fingertip,,2] <- 255
    imwithdot[fingertip,,3] <- 0

    if (triallist[trial]>6){
    imwithdot[rulermin,,1] <- 0
    imwithdot[rulermin,,2] <- 255
    imwithdot[rulermin,,3] <- 255
    
    imwithdot[rulermax,,1] <- 255
    imwithdot[rulermax,,2] <- 0
    imwithdot[rulermax,,3] <- 0
    }
    writeJPEG(imwithdot/255,paste0('local/fingerimages/P',subjID,'im',((cond-1)*6) + condcount[cond],'.jpg'))

          }
    
  }

  allout[s,,] <- output
  
}
save(file='local/allprocessed.RData',list=c('ratings','allout'))
}

if (processdata==0){
  if (!file.exists('local/allprocessed.RData')){
  osfproject <- osf_retrieve_node("b9s48")
  osffiles <- osf_ls_files(osfproject)
  fid <- which(osffiles$name=='allprocessed.RData')
  osf_download(osffiles[fid,], path='local/',progress=TRUE)
  }
  load('local/allprocessed.RData')
}

```

```{r processdata, include=FALSE}

# Reorganisation of data

meansettings <- apply(allout,1:2,mean,na.rm=TRUE)
includedindices <- which(ratings[,1] %in% includedsubjects)
allsettings <- meansettings[includedindices,]

normsettings <- allsettings
for (n in 1:3){
normsettings[,(n*3)-2] <- normsettings[,(n*3)-2] - allsettings[,(n*3)-2]
normsettings[,(n*3)-1] <- normsettings[,(n*3)-1] - allsettings[,(n*3)-2]
normsettings[,(n*3)] <- normsettings[,(n*3)] - allsettings[,(n*3)-2]
}

#set column names
colnames(allsettings) <- c("RightDotBaseline", "RightDotV/VA", "RightDotVT/VTA", 
                           "LeftDotBaseline", "LeftDotV/VA", "LeftDotVT/VTA", 
                           "RulerBaseline", "RulerV/VA", "RulerVT/VTA")

colnames(normsettings) <- c("RightDotBaseline", "RightDotV/VA", "RightDotVT/VTA", 
                            "LeftDotBaseline", "LeftDotV/VA", "LeftDotVT/VTA", 
                            "RulerBaseline", "RulerV/VA", "RulerVT/VTA")

#Extract the needed columns
allsettingsright <- allsettings[,c(1:3)]
allsettingsleft <- allsettings[,c(4:6)]
allsettingsruler <- allsettings[,c(7:9)]

normsettingsright <- normsettings[,c(2:3)]
normsettingsleft <- normsettings[,c(5:6)]
normsettingsruler <- normsettings[,c(8:9)]

#Create group labels
Group <- c("No Audio", "Audio", "Audio", "No Audio", "No Audio", 
           "Audio", "Audio", "Audio", "Audio", "No Audio", "Audio", 
           "Audio", "No Audio", "No Audio", "Audio", "No Audio", 
           "Audio", "No Audio", "No Audio", "Audio", "No Audio",
           "No Audio", "Audio", "Audio", "Audio", "Audio", "No Audio",
           "Audio", "No Audio", "Audio", "No Audio", "Audio", "Audio",
           "No Audio", "No Audio", "No Audio", "Audio", "No Audio",
           "No Audio", "No Audio", "No Audio", "No Audio", "Audio", 
           "No Audio")

#create PID
PID <- c(1:44)

#Bind Group and PID Columns
allsettingsright <- cbind(allsettingsright, Group, PID)
allsettingsleft <- cbind(allsettingsleft, Group, PID)
allsettingsruler <- cbind(allsettingsruler, Group, PID)

normsettingsright <- cbind(normsettingsright, Group, PID)
normsettingsleft <- cbind(normsettingsleft, Group, PID)
normsettingsruler <- cbind(normsettingsruler, Group, PID)

#Make data as dataframes
allsettingsrightdf <- as.data.frame(allsettingsright)
allsettingsleftdf <- as.data.frame(allsettingsleft)
allsettingsrulerdf <- as.data.frame(allsettingsruler)

normsettingsrightdf <- as.data.frame(normsettingsright)
normsettingsleftdf <- as.data.frame(normsettingsleft)
normsettingsrulerdf <- as.data.frame(normsettingsruler)

#set PID as factors
allsettingsrightdf$PID <- factor(allsettingsrightdf$PID)
allsettingsleftdf$PID <- factor(allsettingsleftdf$PID)
allsettingsrulerdf$PID <- factor(allsettingsrulerdf$PID)

normsettingsrightdf$PID <- factor(normsettingsrightdf$PID)
normsettingsleftdf$PID <- factor(normsettingsleftdf$PID)
normsettingsrulerdf$PID <- factor(normsettingsrulerdf$PID)

#Format all dataframes for analysis
allsettingsright_formatted <- melt(allsettingsrightdf,
                                   # ID variables - all the variables to keep but not split apart on
                                   id.vars=c("Group", "PID"),
                                   # The source columns
                                   measure.vars=c("RightDotBaseline", "RightDotV/VA", "RightDotVT/VTA" ),
                                   # Name of the destination column that will identify the original
                                   # column that the measurement came from
                                   variable.name="condition",
                                   value.name="score")


# Sort by subject first, then by condition
allsettingsright_formatted <- allsettingsright_formatted[ order(allsettingsright_formatted$PID,
                                                                allsettingsright_formatted$condition), ]

allsettingsleft_formatted <- melt(allsettingsleftdf,
                                   # ID variables - all the variables to keep but not split apart on
                                   id.vars=c("Group", "PID"),
                                   # The source columns
                                   measure.vars=c("LeftDotBaseline", "LeftDotV/VA", "LeftDotVT/VTA" ),
                                   # Name of the destination column that will identify the original
                                   # column that the measurement came from
                                   variable.name="condition",
                                   value.name="score")

allsettingsleft_formatted <- allsettingsleft_formatted[ order(allsettingsleft_formatted$PID,
                                                                allsettingsleft_formatted$condition), ]

allsettingsruler_formatted <- melt(allsettingsrulerdf,
                                   # ID variables - all the variables to keep but not split apart on
                                   id.vars=c("Group", "PID"),
                                   # The source columns
                                   measure.vars=c("RulerBaseline", "RulerV/VA", "RulerVT/VTA" ),
                                   # Name of the destination column that will identify the original
                                   # column that the measurement came from
                                   variable.name="condition",
                                   value.name="score")

allsettingsruler_formatted <- allsettingsruler_formatted[ order(allsettingsruler_formatted$PID,
                                                                allsettingsruler_formatted$condition), ]

normsettingsright_formatted <- melt(normsettingsrightdf,
                                   # ID variables - all the variables to keep but not split apart on
                                   id.vars=c("Group", "PID"),
                                   # The source columns
                                   measure.vars=c("RightDotV/VA", "RightDotVT/VTA" ),
                                   # Name of the destination column that will identify the original
                                   # column that the measurement came from
                                   variable.name="condition",
                                   value.name="score")

normsettingsright_formatted <- normsettingsright_formatted[ order(normsettingsright_formatted$PID,
                                                                normsettingsright_formatted$condition), ]

normsettingsleft_formatted <- melt(normsettingsleftdf,
                                    # ID variables - all the variables to keep but not split apart on
                                    id.vars=c("Group", "PID"),
                                    # The source columns
                                    measure.vars=c("LeftDotV/VA", "LeftDotVT/VTA" ),
                                    # Name of the destination column that will identify the original
                                    # column that the measurement came from
                                    variable.name="condition",
                                    value.name="score")

normsettingsleft_formatted <- normsettingsleft_formatted[ order(normsettingsleft_formatted$PID,
                                                                  normsettingsleft_formatted$condition), ]

normsettingsruler_formatted <- melt(normsettingsrulerdf,
                                    # ID variables - all the variables to keep but not split apart on
                                    id.vars=c("Group", "PID"),
                                    # The source columns
                                    measure.vars=c("RulerV/VA", "RulerVT/VTA" ),
                                    # Name of the destination column that will identify the original
                                    # column that the measurement came from
                                    variable.name="condition",
                                    value.name="score")

normsettingsruler_formatted <- normsettingsruler_formatted[ order(normsettingsruler_formatted$PID,
                                                                  normsettingsruler_formatted$condition), ]

##Analysis

#convert PID, Group and Condition within subjective data to factors
subjective_data_factors = subjective_data %>%
  convert_as_factor(PID, Group, Condition)

```

```{r analysis, include=FALSE}
## Subjective Analyses ##

### --- ###
#Subjective Illusion Score Analysis
lmeresults1 <- lmer(Illusion.1.Score ~ Condition + Group + Condition*Group + (1 |PID), data=subjective_data_factors)
lmeanova1 <- anova(lmeresults1)
lmeranova1 <- ranova(lmeresults1) #random effects

r2_1 <- r.squaredGLMM(lmeresults1) 

#Q-Q Plots
lmeresults1residuals <- resid(lmeresults1) # extract the residuals
qqnorm(lmeresults1residuals) # create the plot
qqline(lmeresults1residuals) # add the diagonal line

# Run Shapiro-Wilk tests
shapiro.test(subjective_data_factors$Illusion.1.Score)

#sort subjective data into new dataframes for posthoc pairwise analyses
filtered.data_Baseline <- data.frame(subjective_data_factors[c(1,4,7,10,13,16,19,22,25,28,31,34,37,40,43,46,49,52,55,58,61,64,67,70,73,76,79,82,85,88,91,94,97,100,103,106,109,112,115,118,121,124,127,130),c(1:13)])
filtered.data_V_VA <- data.frame(subjective_data_factors[c(2,5,8,11,14,17,20,23,26,29,32,35,38,41,44,47,50,53,56,59,62,65,68,71,74,77,80,83,86,89,92,95,98,101,104,107,110,113,116,119,122,125,128,131),c(1:13)])
filtered.data_VT_VTA <- data.frame(subjective_data_factors[c(3,6,9,12,15,18,21,24,27,30,33,36,39,42,45,48,51,54,57,60,63,66,69,72,75,78,81,84,87,90,93,96,99,102,105,108,111,114,117,120,123,126,129,132),c(1:13)])
filtered.data_VA_VT <- data.frame(subjective_data_factors[c(3,5,8,12,15,17,20,23,26,30,32,35,39,42,44,48,50,54,57,59,63,66,68,71,74,77,81,83,87,89,93,95,98,102,105,108,110,114,117,120,123,126,128,132),c(1:13)])

#perform the Holm post-hoc method
ttest1 <- pairwise.t.test(filtered.data_Baseline$Illusion.1.Score, filtered.data_Baseline$Group, p.adj="holm")
ttest2 <- pairwise.t.test(filtered.data_V_VA$Illusion.1.Score, filtered.data_V_VA$Group, p.adj='holm')
ttest3 <- pairwise.t.test(filtered.data_VT_VTA$Illusion.1.Score, filtered.data_VT_VTA$Group, p.adj='holm')
ttest4 <- t.test(filtered.data_VA_VT$Illusion.1.Score ~ filtered.data_VA_VT$Condition, var.equal=TRUE, data = filtered.data_VA_VT)


#Get means for V/VA condition
means1 <- group_by(filtered.data_V_VA, Group) %>%
  summarise(
    count = n(),
    mean = mean(Illusion.1.Score, na.rm = TRUE),
    sd = sd(Illusion.1.Score, na.rm = TRUE)
  )

#Get means for VT/VTA condition
means2 <- group_by(filtered.data_VT_VTA, Group) %>%
  summarise(
    count = n(),
    mean = mean(Illusion.1.Score, na.rm = TRUE),
    sd = sd(Illusion.1.Score, na.rm = TRUE)
  )

#Shapiro-Wilk test for filtered data
shapiro.test(filtered.data_Baseline$Illusion.1.Score)
shapiro.test(filtered.data_V_VA$Illusion.1.Score)
shapiro.test(filtered.data_VT_VTA$Illusion.1.Score)

### --- ###
#Subjective Illusion Score Analysis 2
lmeresults2 <- lmer(Illusion.2.Score ~ Condition + Group + Condition*Group + (1 |PID), data=subjective_data_factors)
anova(lmeresults2)

r2_2 <- r.squaredGLMM(lmeresults2)
r2_2 

#Get means for each condition
meansBL <- group_by(filtered.data_Baseline, Group) %>%
  summarise(
    count = n(),
    mean = mean(Illusion.2.Score, na.rm = TRUE),
    sd = sd(Illusion.2.Score, na.rm = TRUE)
  )
meansVVA <- group_by(filtered.data_V_VA, Group) %>%
  summarise(
    count = n(),
    mean = mean(Illusion.2.Score, na.rm = TRUE),
    sd = sd(Illusion.2.Score, na.rm = TRUE)
  )
meansVTVTA <- group_by(filtered.data_VT_VTA, Group) %>%
  summarise(
    count = n(),
    mean = mean(Illusion.2.Score, na.rm = TRUE),
    sd = sd(Illusion.2.Score, na.rm = TRUE)
  )

lmeresults2residuals <- resid(lmeresults2) # extract the residuals
qqnorm(lmeresults2residuals) # create the plot
qqline(lmeresults2residuals) # add the diagonal line

#Shapiro-Wilk Test
shapiro.test(subjective_data_factors$Illusion.2.Score)

### --- ###
#Subjective Disownership Score Analysis
lmeresults3 <- lmer(Disownership.Average ~ Condition + Group + Condition*Group + (1 |PID), data=subjective_data_factors)
anova(lmeresults3)

r2_3 <- r.squaredGLMM(lmeresults3)
r2_3

#Get means for each condition
meansBLD <- group_by(filtered.data_Baseline, Group) %>%
  summarise(
    count = n(),
    mean = mean(Disownership.Average, na.rm = TRUE),
    sd = sd(Disownership.Average, na.rm = TRUE)
  )
meansVVAD <- group_by(filtered.data_V_VA, Group) %>%
  summarise(
    count = n(),
    mean = mean(Disownership.Average, na.rm = TRUE),
    sd = sd(Disownership.Average, na.rm = TRUE)
  )
meansVTVTAD <- group_by(filtered.data_VT_VTA, Group) %>%
  summarise(
    count = n(),
    mean = mean(Disownership.Average, na.rm = TRUE),
    sd = sd(Disownership.Average, na.rm = TRUE)
  )

#Q-Q Plots
lmeresults3residuals <- resid(lmeresults3) # extract the residuals
qqnorm(lmeresults3residuals) # create the plot
qqline(lmeresults3residuals) # add the diagonal line

#Shapiro-Wilk Test
shapiro.test(subjective_data_factors$Disownership.Average)

#Post Hoc Tests
pairwise.t.test(filtered.data_Baseline$Disownership.Average, filtered.data_Baseline$Group, p.adj="holm")
pairwise.t.test(filtered.data_V_VA$Disownership.Average, filtered.data_V_VA$Group, p.adj='holm')
pairwise.t.test(filtered.data_VT_VTA$Disownership.Average, filtered.data_VT_VTA$Group, p.adj='holm')


#Shapiro-Wilk Test
shapiro.test(filtered.data_Baseline$Disownership.Average)
shapiro.test(filtered.data_V_VA$Disownership.Average)
shapiro.test(filtered.data_VT_VTA$Disownership.Average)

### --- ###
#Subjecitve Data Control Analysis
lmeresults4 <- lmer(Control.Average ~ Condition + Group + Condition*Group + (1 |PID), data=subjective_data_factors)
anova(lmeresults4)

r2_4 <- r.squaredGLMM(lmeresults4)
r2_4

#Get means for each condition
meansBLC <- group_by(filtered.data_Baseline, Group) %>%
  summarise(
    count = n(),
    mean = mean(Control.Average, na.rm = TRUE),
    sd = sd(Control.Average, na.rm = TRUE)
  )
meansVVAC <- group_by(filtered.data_V_VA, Group) %>%
  summarise(
    count = n(),
    mean = mean(Control.Average, na.rm = TRUE),
    sd = sd(Control.Average, na.rm = TRUE)
  )
meansVTVTAC <- group_by(filtered.data_VT_VTA, Group) %>%
  summarise(
    count = n(),
    mean = mean(Control.Average, na.rm = TRUE),
    sd = sd(Control.Average, na.rm = TRUE)
  )

#Q-Q Plots
lmeresults4residuals <- resid(lmeresults4) # extract the residuals
qqnorm(lmeresults4residuals) # create the plot
qqline(lmeresults4residuals) # add the diagonal line

#Shapiro-Wilk Test
shapiro.test(subjective_data_factors$Control.Average)

```

```{r objectiveanalysis, include=FALSE}

## Objective analyses ##

### --- ###
#Objective Positive Control Right Dot

#set score to numeric
allsettingsright_formatted$score = as.numeric(as.character(allsettingsright_formatted$score))

#invert direction of score
allsettingsright_formatted$score <- allsettingsright_formatted$score*(-1)

#divide score (pixels) by 28 to give cm's
allsettingsright_formatted$score <- allsettingsright_formatted$score/28

#analysis from formatted data
lmeresults5 <- lmer(score ~ condition + Group + condition*Group + (1 |PID), data=allsettingsright_formatted)
lmeanova5 <- anova(lmeresults5)
lmeranova5 <- ranova(lmeresults5)

r2_5 <- r.squaredGLMM(lmeresults5)
r2_5

#Q-Q Plots
lmeresults5residuals <- resid(lmeresults5) # extract the residuals
qqnorm(lmeresults5residuals) # create the plot
qqline(lmeresults5residuals) # add the diagonal line

#Shapiro-Wilk Test
shapiro.test(allsettingsright_formatted$score)

# POST HOC TEST
posthoc5 <- summary(emmeans(lmeresults5, list(pairwise ~ condition), adjust = "tukey"))

### --- ###
#Objective Positive Control Left Dot

#set score to numeric
allsettingsleft_formatted$score = as.numeric(as.character(allsettingsleft_formatted$score))

#invert direction of score
allsettingsleft_formatted$score <- allsettingsleft_formatted$score*(-1)

#divide score (pixels) by 28 to give cm's
allsettingsleft_formatted$score <- allsettingsleft_formatted$score/28

#analysis from formatted data
lmeresults6 <- lmer(score ~ condition + Group + condition*Group + (1 |PID), data=allsettingsleft_formatted)
lmeanova6 <- anova(lmeresults6)
lmeranova6 <- ranova(lmeresults6)

r2_6 <- r.squaredGLMM(lmeresults6)
r2_6


#Q-Q Plots
lmeresults6residuals <- resid(lmeresults6) # extract the residuals
qqnorm(lmeresults6residuals) # create the plot
qqline(lmeresults6residuals) # add the diagonal line

#Shapiro-Wilk Test
shapiro.test(allsettingsleft_formatted$score)

#Post Hoc Test
posthoc6 <- summary(emmeans(lmeresults6, list(pairwise ~ condition), adjust = "tukey"))

### --- ###
#Objective Positive Control Ruler

#set score to numeric
allsettingsruler_formatted$score = as.numeric(as.character(allsettingsruler_formatted$score))

#times score (pixels) by 1.8 to give cm's
allsettingsruler_formatted$score <- allsettingsruler_formatted$score*(1.8)

#remove outliers
allsettingsruler_formatted$score[allsettingsruler_formatted$score < quantile(allsettingsruler_formatted$score, 0.25) 
                           - 1.5*IQR(allsettingsruler_formatted$score) | 
                             allsettingsruler_formatted$score > quantile(allsettingsruler_formatted$score, 0.75) 
                           + 1.5*IQR(allsettingsruler_formatted$score)] <- NA 

#analysis from formatted data
lmeresults7 <- lmer(score ~ condition + Group + condition*Group + (1 |PID), data=allsettingsruler_formatted)
lmeanova7 <- anova(lmeresults7)
lmeranova7 <- ranova(lmeresults7)

r2_7 <- r.squaredGLMM(lmeresults7)
r2_7

#Q-Q Plots
lmeresults7residuals <- resid(lmeresults7) # extract the residuals
qqnorm(lmeresults7residuals) # create the plot
qqline(lmeresults7residuals) # add the diagonal line

#Shapiro-Wilk Test
shapiro.test(allsettingsruler_formatted$score)

#Post Hoc Test
posthoc7 <- summary(emmeans(lmeresults7, list(pairwise ~ condition), adjust = "tukey"))

### --- ###
#Objective Right Dot

#set score to numeric
normsettingsright_formatted$score = as.numeric(as.character(normsettingsright_formatted$score))

#invert direction of score
normsettingsright_formatted$score <- normsettingsright_formatted$score*(-1)

#divide score (pixels) by 28 to give cm's
normsettingsright_formatted$score <- normsettingsright_formatted$score/28

#analysis from formatted data
lmeresults8 <- lmer(score ~ condition + Group + condition*Group + (1 |PID), data=normsettingsright_formatted)
lmeanova8 <- anova(lmeresults8)
lmeranova8 <- ranova(lmeresults8)

r2_8 <- r.squaredGLMM(lmeresults8)
r2_8

#Q-Q Plots
lmeresults8residuals <- resid(lmeresults8) # extract the residuals
qqnorm(lmeresults8residuals) # create the plot
qqline(lmeresults8residuals) # add the diagonal line

#Shapiro-Wilk Test
shapiro.test(normsettingsright_formatted$score)

### --- ###
#Objective Left Dot

#set score to numeric
normsettingsleft_formatted$score = as.numeric(as.character(normsettingsleft_formatted$score))

#invert direction of score
normsettingsleft_formatted$score <- normsettingsleft_formatted$score*(-1)

#divide score (pixels) by 28 to give cm's
normsettingsleft_formatted$score <- normsettingsleft_formatted$score/28

#analysis from formatted data
lmeresults9 <- lmer(score ~ condition + Group + condition*Group + (1 |PID), data=normsettingsleft_formatted)
lmeanova9 <- anova(lmeresults9)
lmeranova9 <- ranova(lmeresults9)

r2_9 <- r.squaredGLMM(lmeresults9)
r2_9

#Q-Q Plots
lmeresults9residuals <- resid(lmeresults9) # extract the residuals
qqnorm(lmeresults9residuals) # create the plot
qqline(lmeresults9residuals) # add the diagonal line

#Shapiro-Wilk Test
shapiro.test(normsettingsleft_formatted$score)

#Get Means and SDs
means9 <- group_by(normsettingsleft_formatted, condition) %>%
  summarise(
    count = n(),
    mean = mean(score, na.rm = TRUE),
    sd = sd(score, na.rm = TRUE)
  )


#normplots <- grid.arrange(normrightdotplot, normleftdotplot, ncol=2)

#ggsave('figures/normplots.pdf', normplots, width = 9, height = 5, dpi = 300)
### --- ###
#Objective Ruler

#set score to numeric
normsettingsruler_formatted$score = as.numeric(as.character(normsettingsruler_formatted$score))

#times score (pixels) by 1.8 to give cm's
normsettingsruler_formatted$score <- normsettingsruler_formatted$score*(1.8)

#remove outliers
normsettingsruler_formatted$score[normsettingsruler_formatted$score < quantile(normsettingsruler_formatted$score, 0.25) 
                           - 1.5*IQR(normsettingsruler_formatted$score) | 
                             normsettingsruler_formatted$score > quantile(normsettingsruler_formatted$score, 0.75) 
                           + 1.5*IQR(normsettingsruler_formatted$score)] <- NA 

#analysis from formatted data
lmeresults10 <- lmer(score ~ condition + Group + condition*Group + (1 |PID), data=normsettingsruler_formatted)
lmeanova10 <- anova(lmeresults10)
lmeranova10 <- ranova(lmeresults10)

r2_10 <- r.squaredGLMM(lmeresults10)
r2_10

#Q-Q Plots
lmeresults10residuals <- resid(lmeresults10) # extract the residuals
qqnorm(lmeresults10residuals) # create the plot
qqline(lmeresults10residuals) # add the diagonal line

#Shapiro-Wilk Test
shapiro.test(normsettingsruler_formatted$score)

```

```{r plotillusion, include=FALSE}

#plot data
Illusion1plot <- ggboxplot(subjective_data_factors, x = "Condition", y = "Illusion.1.Score", 
                           color = "Group", palette = c("#00AFBB", "#FC4E07"), 
                           add = "jitter" )+ ylab("Subjective Illusion Score") + xlab("Condition") + geom_bracket(
    xmin = 1.80, xmax = 2.20, y.position = 105,
    label = "*")



ggsave('figures/Illusion1.pdf', Illusion1plot, width = 7, height = 5, dpi = 300)

#Plot Data
Illusion2plot <- ggboxplot(subjective_data_factors, x = "Condition", y = "Illusion.2.Score", color = "Group",palette = c("#00AFBB", "#FC4E07"), add = "jitter" )

ggsave('figures/Illusion2.pdf', Illusion2plot, width = 5, height = 5, dpi = 300)

```

```{r plotdisownership, include=FALSE}

#plot data
Disownershipplot <- ggboxplot(subjective_data_factors, x = "Condition", y = "Disownership.Average", color = "Group", palette = c("#00AFBB", "#FC4E07"), add = "jitter" )

ggsave('figures/disownershipaverage.pdf', Disownershipplot, width = 5, height = 5, dpi = 300)

```

```{r plotcontrol, include=FALSE}

#plot data
Controlplot <- ggboxplot(subjective_data_factors, x = "Condition", y = "Control.Average", color = "Group",palette = c("#00AFBB", "#FC4E07"), add = "jitter" )

ggsave('figures/controlaverage.pdf', Controlplot, width = 5, height = 5, dpi = 300)

```

```{r plotdottask, include=FALSE}

#Plot Data
PCrightdotplot <- ggboxplot(allsettingsright_formatted, x = "condition", y = "score", 
                            color = "Group", palette = c("#00AFBB", "#FC4E07"), 
                            add = "jitter")+ ylab("Dot Touch Accuracy (cm)") + xlab("Condition") + geom_bracket(
    xmin = c("RightDotBaseline", "RightDotBaseline"), xmax = c("RightDotV/VA", "RightDotVT/VTA"),
    y.position = c(2.2, 2.9), label = c("***", "***"))

PCrightdotplot <- ggpar(PCrightdotplot, ylim = c(-4, 3))

ggsave('figures/PCrightdot.pdf', PCrightdotplot, width = 5, height = 5, dpi = 300)


#Plot Data
PCleftdotplot <- ggboxplot(allsettingsleft_formatted, x = "condition", y = "score", 
                           color = "Group", palette = c("#00AFBB", "#FC4E07"), 
                           add = "jitter" )+ ylab("Dot Touch Accuracy (cm)") + xlab("Condition") + geom_bracket(
    xmin = c("LeftDotBaseline", "LeftDotBaseline"), xmax = c("LeftDotV/VA", "LeftDotVT/VTA"),
    y.position = c(2.2, 2.9), label = c("***", "*"))

PCleftdotplot <- ggpar(PCleftdotplot, ylim = c(-4, 3))

ggsave('figures/PCleftdot.pdf', PCleftdotplot, width = 5, height = 5, dpi = 300)

```

```{r plotruler, include=FALSE}

#Plot Data
PCrulerplot <- ggboxplot(allsettingsruler_formatted, x = "condition", y = "score", 
                         color = "Group", palette = c("#00AFBB", "#FC4E07"), 
                         add = "jitter" )+ ylab("Percieved Length (cm)") + xlab("Condition") + geom_bracket(
    xmin = c("RulerBaseline", "RulerBaseline"), xmax = c("RulerV/VA", "RulerVT/VTA"),
    y.position = c(3.3, 4), label = c("***", "***"))

PCrulerplot <- ggpar(PCrulerplot, ylim = c(-5, 5))


ggsave('figures/PCruler.pdf', PCrulerplot, width = 5, height = 5, dpi = 300)

```

```{r plotnormalizeddots, include=FALSE}

pdf('figures/normdots.pdf',width=11,height=5)

par(mar=c(0,0,0,0))
plot(x=NULL,y=NULL,axes=FALSE, ann=FALSE, xlim=c(0,1), ylim=c(0,1)) 
arrows(0.48,0.55,x1=0.48,y1=0.9,length=0.1,angle=30,lwd=3)
arrows(0.48,0.45,x1=0.48,y1=0.1,length=0.1,angle=30,lwd=3)
text(0.51,0.725,'Underestimated length',srt=90)
text(0.51,0.275,'Overestimated length',srt=90)

#Plot Data
normrightdotplot <- ggboxplot(normsettingsright_formatted, x = "condition", y = "score", 
                              color = "Group", palette = c("#00AFBB", "#FC4E07"), 
                              add = "jitter")+ ylab("Dot Touch Accuracy (cm)") + xlab("Condition")
normrightdotplot <- ggpar(normrightdotplot, ylim = c(-2, 2))


#Plot Data
normleftdotplot <- ggboxplot(normsettingsleft_formatted, x = "condition", y = "score", 
                             color = "Group", palette = c("#00AFBB", "#FC4E07"), 
                             add = "jitter" )+ ylab("Dot Touch Accuracy (cm)") + xlab("Condition") + geom_bracket(
    xmin = "LeftDotV/VA", xmax = "LeftDotVT/VTA", y.position = 1.75,
    label = "***")
normleftdotplot <- ggpar(normleftdotplot, ylim = c(-2, 2))

grid.arrange(normleftdotplot,NULL,normrightdotplot, ncol=3, widths=c(5,1,5), newpage=FALSE)

dev.off()

```

```{r plotnormalizedruler, include=FALSE}

pdf('figures/normruler.pdf',width=6,height=5)

par(mar=c(0,0,0,0))
plot(x=NULL,y=NULL,axes=FALSE, ann=FALSE, xlim=c(0,1), ylim=c(0,1)) 
arrows(0.9,0.5,x1=0.9,y1=0.8,length=0.1,angle=30,lwd=3)
arrows(0.9,0.4,x1=0.9,y1=0.1,length=0.1,angle=30,lwd=3)
text(0.95,0.65,'Perceived longer',srt=90)
text(0.95,0.25,'Perceived shorter',srt=90)

#Plot Data
normrulerplot <- ggboxplot(normsettingsruler_formatted, x = "condition", y = "score", color = "Group", palette = c("#00AFBB", "#FC4E07"), add = "jitter" )+ ylab("Relative Percieved Length (cm)") + xlab("Condition")

grid.arrange(normrulerplot, ncol=2, widths=c(5,1), newpage=FALSE)


dev.off()

```

Hypothesis 1 predicted that adding a non-naturalistic auditory input to augmented reality resizing illusions that is consistent with the visual and tactile manipulations of stretching a finger, would increase subjective illusion strength. We measured this via a subjective illusory experience questionnaire, for (1a) visual and (1b) visuo-tactile manipulations, with results shown in Figure 2. A random intercepts linear mixed effects model with fixed effects of group and condition, and a random effect of participant, was used to assess if there was an effect of group (Audio vs Non-Audio) on subjective illusory experience score in the V/VA and VT/VTA conditions. Analysis showed a statistically significant interaction between condition and group *F*(`r round(lmeanova1$NumDF[3], digits = 2)`, `r round(lmeanova1$DenDF[3], digits = 2)`) = `r round(lmeanova1$"F value"[3], digits = 2)`, *p* = `r format.pval(lmeanova1$"Pr(>F)"[3], digits = 1, eps = 0.001, nsmall = 3)`. Simple main effects analysis showed that both condition (*F*(`r round(lmeanova1$NumDF[1], digits = 2)`, `r round(lmeanova1$DenDF[1], digits = 2)`) = `r round(lmeanova1$"F value"[1], digits = 2)`, *p* = `r format.pval(lmeanova1$"Pr(>F)"[1], digits = 1, eps = 0.001, nsmall = 3)`) and group (*F*(`r round(lmeanova1$NumDF[2], digits = 2)`, `r round(lmeanova1$DenDF[2], digits = 2)`) = `r round(lmeanova1$"F value"[2], digits = 2)`,*p* = `r format.pval(lmeanova1$"Pr(>F)"[2], digits = 1, eps = 0.001, nsmall = 3)`) had a significant effect on subjective illusory experience score. The random effects term did not make a significant contribution to the fit of the model (*p* = `r format.pval(lmeranova1$"Pr(>Chisq)"[2], digits = 1, eps = 0.001, nsmall = 3)`), and the marginal R² value was `r round(r2_1[1]*100, digits = 2)`% with the conditional R² value being `r round(r2_1[2]*100, digits = 2)`%. Post hoc pairwise t tests with Holm correction for multiple comparisons found that participants experienced a significantly stronger illusion in the VA condition (M=`r round(means1$mean[1])`, SD=`r round(means1$sd[1])`) compared to the V condition (M=`r round(means1$mean[2])`, SD=`r round(means1$sd[2])`) (*p* = `r format.pval(ttest2$"p.value"[1], digits = 1, eps = 0.001, nsmall = 3)`) and found no difference in illusion strength when comparing the VT/VTA conditions (*p* = `r format.pval(ttest3$"p.value"[1], digits = 1, eps = 0.001, nsmall = 3)`), indicating that the addition of non-naturalistic auditory input significantly affected subjective illusory experience in the unimodal visual condition, but had no effect on the combined visuo-tactile condition. Participants reported mean scores of above 50 (the neutral point of the scale) in all conditions for the second item on the subjective questionnaire, indicating experience of ownership of the seen hand in all conditions, whilst reporting mean scores for disownership and control statements below 50, indicating no average disownership of the hand and no average violations of the control statements (results can be seen in supplementary materials S1 – S3).

```{r figure2, fig.cap="Subjective Illusory Experience Score for Baseline, V/VA and VT/VTA conditions. Group is indicated by colour, with red showing the no audio group and blue showing the audio group.", fig.align='center', echo=FALSE}

knitr::include_graphics('figures/Illusion1.pdf')

```

Positive control analyses were run on the performance data to check that we were able to see an effect of the illusion with the dot touch and ruler judgement tasks. Positive control data plots can be seen in supplementary materials (S4-S6). A random intercepts linear mixed effects model with fixed effects of group and condition, and a random effect of participant, was used to assess if the V/VA and VT/VTA conditions showed a significant difference in performance on dot touch and ruler judgement data compared to the baseline condition. First regarding the right dot touch data, we found a significant effect of condition *F*(`r round(lmeanova5$NumDF[1], digits = 2)`, `r round(lmeanova5$DenDF[1], digits = 2)`) = `r round(lmeanova5$"F value"[1], digits = 2)`, *p* = `r format.pval(lmeanova5$"Pr(>F)"[1], digits = 1, eps = 0.001, nsmall = 3)`, with the random effects term making a significant contribution to the fit of the model (*p* = `r format.pval(lmeranova5$"Pr(>Chisq)"[2], digits = 1, eps = 0.001, nsmall = 3)`), with the marginal R² value being `r round(r2_5[1]*100, digits = 2)`% and the conditional R² value being `r round(r2_5[2]*100, digits = 2)`%. Post hoc Tukey’s Test for multiple pairwise comparisons found that participants placed their finger significantly lower than the dot in the V/VA condition (*p* = `r format.pval(posthoc5$"pairwise differences of condition"$p.value[1], digits = 1, eps = 0.001, nsmall = 3)`, 95% C.I. = [`r round(posthoc5$"emmeans of condition"$lower.CL[2], digits = 2)`, `r round(posthoc5$"emmeans of condition"$upper.CL[2], digits = 2)`], M = `r round(posthoc5$"emmeans of condition"$emmean[2], digits = 2)`) and the VT/VTA condition (*p* = `r format.pval(posthoc5$"pairwise differences of condition"$p.value[2], digits = 1, eps = 0.001, nsmall = 3)`, 95% C.I. = [`r round(posthoc5$"emmeans of condition"$lower.CL[3], digits = 2)`, `r round(posthoc5$"emmeans of condition"$upper.CL[3], digits = 2)`], M = `r round(posthoc5$"emmeans of condition"$emmean[3], digits = 2)`) compared to the baseline condition (95% C.I. = [`r round(posthoc5$"emmeans of condition"$lower.CL[1], digits = 2)`, `r round(posthoc5$"emmeans of condition"$upper.CL[1], digits = 2)`], M = `r round(posthoc5$"emmeans of condition"$emmean[1], digits = 2)`), indicating an effect of the finger stretching manipulation was by this performance measure; participants experienced their index finger as significantly longer under these manipulation conditions and this subsequently produced a measurable effect upon body schema.  Secondly, regarding the left dot touch data, we found a significant effect of condition *F*(`r round(lmeanova6$NumDF[1], digits = 2)`, `r round(lmeanova6$DenDF[1], digits = 2)`) = `r round(lmeanova6$"F value"[1], digits = 2)`, *p* = `r format.pval(lmeanova6$"Pr(>F)"[1], digits = 1, eps = 0.001, nsmall = 3)`, with the random effects term making a significant contribution to the fit of the model (*p* = `r format.pval(lmeranova6$"Pr(>Chisq)"[2], digits = 1, eps = 0.001, nsmall = 3)`), with the marginal R² value being `r round(r2_6[1]*100, digits = 2)`% and the conditional R² value being `r round(r2_6[2]*100, digits = 2)`%. Post hoc Tukey’s Test for multiple pairwise comparisons found that participants placed their finger significantly lower than the dot in the V/VA condition (*p* = `r format.pval(posthoc6$"pairwise differences of condition"$p.value[1], digits = 1, eps = 0.001, nsmall = 3)`, 95% C.I. = [`r round(posthoc6$"emmeans of condition"$lower.CL[2], digits = 2)`, `r round(posthoc6$"emmeans of condition"$upper.CL[2], digits = 2)`], M = `r round(posthoc6$"emmeans of condition"$emmean[2], digits = 2)`) and the VT/VTA condition (*p* = `r format.pval(posthoc6$"pairwise differences of condition"$p.value[2], digits = 1, eps = 0.001, nsmall = 3)`, 95% C.I. = [`r round(posthoc6$"emmeans of condition"$lower.CL[3], digits = 2)`, `r round(posthoc6$"emmeans of condition"$upper.CL[3], digits = 2)`], M = `r round(posthoc6$"emmeans of condition"$emmean[3], digits = 2)`) compared to the baseline condition (95% C.I. = [`r round(posthoc6$"emmeans of condition"$lower.CL[1], digits = 2)`, `r round(posthoc6$"emmeans of condition"$upper.CL[1], digits = 2)`], M = `r round(posthoc6$"emmeans of condition"$emmean[1], digits = 2)`). Finally, regarding the ruler judgement data, we found a significant effect of condition *F*(`r round(lmeanova7$NumDF[1], digits = 2)`, `r round(lmeanova7$DenDF[1], digits = 2)`) = `r round(lmeanova7$"F value"[1], digits = 2)`, *p* = `r format.pval(lmeanova7$"Pr(>F)"[1], digits = 1, eps = 0.001, nsmall = 3)`, with the random effects term making a significant contribution to the fit of the model (*p* = `r format.pval(lmeranova7$"Pr(>Chisq)"[2], digits = 1, eps = 0.001, nsmall = 3)`), with the marginal R² value being `r round(r2_7[1]*100, digits = 2)`% and the conditional R² value being `r round(r2_7[2]*100, digits = 2)`%. Post hoc Tukey’s Test for multiple pairwise comparisons found that participants judged their finger to be significantly longer in the V/VA condition (*p* = `r format.pval(posthoc7$"pairwise differences of condition"$p.value[2], digits = 1, eps = 0.001, nsmall = 3)`, 95% C.I. = [`r round(posthoc7$"emmeans of condition"$lower.CL[2], digits = 2)`, `r round(posthoc7$"emmeans of condition"$upper.CL[2], digits = 2)`], M = `r round(posthoc7$"emmeans of condition"$emmean[2], digits = 2)`) and the VT/VTA condition (*p* = `r format.pval(posthoc7$"pairwise differences of condition"$p.value[2], digits = 1, eps = 0.001, nsmall = 3)`, 95% C.I. = [`r round(posthoc7$"emmeans of condition"$lower.CL[3], digits = 2)`, `r round(posthoc7$"emmeans of condition"$upper.CL[3], digits = 2)`], M = `r round(posthoc7$"emmeans of condition"$emmean[3], digits = 2)`) compared to the baseline condition (95% C.I. = [`r round(posthoc7$"emmeans of condition"$lower.CL[1], digits = 2)`, `r round(posthoc7$"emmeans of condition"$upper.CL[1], digits = 2)`], M = `r round(posthoc7$"emmeans of condition"$emmean[1], digits = 2)`), indicating the effectiveness of the experimental conditions to make the participants experience a longer finger. 

We then addressed hypothesis 2, that the addition of auditory input would heighten ability on performance tasks, using a dot touch proprioceptive drift task as an index of body schema for (2a) visual and (2b) visuotactile manipulations (see Figure 3). Again, a random intercepts linear mixed effects model with fixed effects being group and condition, and a random effect of participant, was used to assess if there was an effect of group (Audio Vs Non-Audio) on dot touch data in the V/VA and VT/VTA conditions. Analysis on right dot touch data (Figure 3b) showed no significant interaction between condition and group *F*(`r round(lmeanova8$NumDF[3], digits = 2)`, `r round(lmeanova8$DenDF[3], digits = 2)`) = `r round(lmeanova8$"F value"[3], digits = 2)`, *p* = `r format.pval(lmeanova8$"Pr(>F)"[3], digits = 1, eps = 0.001, nsmall = 3)` and simple main effects showed no effect of condition (*p* = `r format.pval(lmeanova8$"Pr(>F)"[1], digits = 1, eps = 0.001, nsmall = 3)`) or group (*p* = `r format.pval(lmeanova8$"Pr(>F)"[2], digits = 1, eps = 0.001, nsmall = 3)`). The random effects term did not make a significant contribution to the fit of the model (*p* = `r format.pval(lmeranova8$"Pr(>Chisq)"[2], digits = 1, eps = 0.001, nsmall = 3)`), and the marginal R² value was `r round(r2_8[1]*100, digits = 2)`% with the conditional R² value being `r round(r2_8[2]*100, digits = 2)`%. Analysis on left dot touch data (Figure 3a) showed no significant interaction between condition and group *F*(`r round(lmeanova9$NumDF[3], digits = 2)`, `r round(lmeanova8$DenDF[3], digits = 2)`) = `r round(lmeanova9$"F value"[3], digits = 2)`, *p* = `r format.pval(lmeanova9$"Pr(>F)"[3], digits = 1, eps = 0.001, nsmall = 3)` whilst simple main effects showed no effect of group (*p* = `r format.pval(lmeanova9$"Pr(>F)"[2], digits = 1, eps = 0.001, nsmall = 3)`) but did show an effect of condition (*p* = `r format.pval(lmeanova9$"Pr(>F)"[1], digits = 1, eps = 0.001, nsmall = 3)`), with participants placing their finger significantly lower in the V/VA condition (M =`r round(means9$mean[1], digits = 2)`, SD =`r round(means9$sd[1], digits = 2)`) compared to the VT/VTA condition (M =`r round(means9$mean[2], digits = 2)`, SD =`r round(means9$sd[2], digits = 2)`), indicating that participants experienced a longer finger in the V/VA condition compared to the VT/VTA condition. The random effects term made a significant contribution to the fit of the model (*p* = `r format.pval(lmeranova9$"Pr(>Chisq)"[2], digits = 1, eps = 0.001, nsmall = 3)`), and the marginal R² value was `r round(r2_9[1]*100, digits = 2)`% with the conditional R² value being `r round(r2_9[2]*100, digits = 2)`%.

```{r figure3, fig.cap="Dot Touch Data in relative centimetres for V/VA and VT/VTA conditions for both left and right hand data. Group is indicated by colour, with red showing the no audio group and blue showing the audio group. Arrows denote the direction of finger length estimation.", fig.show='hold', fig.align='center', out.width="95%", echo=FALSE}

knitr::include_graphics(c('figures/normdots.pdf'))

```

Finally, we assesed hypothesis 3, that the addition of auditory input would heighten ability on a performance task measured using a ruler judgement task that indexes body image for (3a) visual and (3b) visuotactile manipulations (see Figure 4). Again, a random intercepts linear mixed effects model with fixed effects being group and condition, and a random effect of participant, was used to assess if there was an effect of group (Audio Vs Non-Audio) on ruler judgement data in the V/VA and VT/VTA conditions. Analysis showed no significant interaction between condition and group *F*(`r round(lmeanova10$NumDF[3], digits = 2)`, `r round(lmeanova10$DenDF[3], digits = 2)`) = `r round(lmeanova10$"F value"[3], digits = 3)`, *p* = `r format.pval(lmeanova10$"Pr(>F)"[3], digits = 1, eps = 0.001, nsmall = 3)` and simple main effects showed no effect of condition (*p* = `r format.pval(lmeanova10$"Pr(>F)"[1], digits = 1, eps = 0.001, nsmall = 3)`) or group (*p* = `r format.pval(lmeanova10$"Pr(>F)"[2], digits = 1, eps = 0.001, nsmall = 3)`). The random effects term did make a significant contribution to the fit of the model (*p* = `r format.pval(lmeranova10$"Pr(>Chisq)"[2], digits = 1, eps = 0.001, nsmall = 3)`), and the marginal R² value was `r round(r2_10[1]*100, digits = 2)`% with the conditional R² value being `r round(r2_10[2]*100, digits = 2)`%.

```{r figure4, fig.cap="Ruler Judgement data in relative centimeters for V/VA and VT/VTA conditions. Group is indicated by colour, with red showing the no audio group and blue showing the audio group. Arrows denote direction of perceived finger length.", fig.show='hold', fig.align='center', out.width="50%", echo=FALSE}

knitr::include_graphics(c('figures/normruler.pdf'))

```

## 4. Discussion {.unnumbered}

This study sought to understand what impact the addition of non-naturalistic auditory input would have on traditional visuo-tactile and unimodal-visual hand-based resizing illusions. Our results showed that the addition of non-naturalistic auditory input, that was consistent with the resizing illusion, increased subjective experience of the illusion in the traditional unimodal-visual condition, with participants experiencing a significantly stronger illusion in the visual-auditory condition as compared to the visual-only condition, supporting our first hypothesis. However, we found no facilitatory effects of auditory input for subjective experience of illusion strength within the combined visuo-tactile condition, or for either of the performance tasks, which was in opposition to our remaining hypotheses and served to highlight a potential discordance between the conscious subjective experience of resizing illusions compared to more unconscious performance-based responses. 

The subjective findings showed that participants in the audio group rated their experience of the illusion to be greater in the visual condition compared to the non-audio group, showing the expected effect of multisensory integration heightening the experience of a stimulus. There was, however, no difference between the audio group and the non-audio group in the visuo-tactile condition, likely due to ceiling effects, wherein the addition of auditory input to visuotactile input did not increase subjective experience of the illusion. The combination of visual and tactile inputs in the VT condition resulted in a significantly higher mean subjective illusion score (*t*(`r round(ttest4$parameter[1])`) = `r round(ttest4$statistic[1], digits = 2)`, *p* = `r format.pval(ttest4$p.value[1], digits = 1, eps = 0.001, nsmall = 3)`) (M = `r round(means2$mean[2])`, SD = `r round(means2$sd[2], digits = 1)`) than in the visual-auditory conditions (M = `r round(means1$mean[1], digits = 1)`, SD = `r round(means1$sd[1])`), demonstrating that the combination of two different senses produces differing levels of subjective experience of the illusion, with visuo-tactile surpassing that of visual-auditory manipulations. It is likely that this increased subjective experience within the visuotactile condition is due to the specific nature of the tactile and proprioceptive inputs, which are thought to be specific to the bodily experience, whereas senses such as vision and audition are experienced not only in relation to our body but also relating to objects in the external world [@tsakiris2010a;@botvinick1998a]. Therefore, it is plausible that including a sense that is integral to our bodily experience, such as tactile input, would have a greater effect on body illusions in comparison to less embodied senses such as auditory input. This is supported by @ernst2002a, who proposed the theory that sensory inputs are combined in a statistically optimal fashion based on their reliability in reflecting the accuracy of a given stimulus. In the current resizing illusions, Ernst and Banks’ theory explains our findings of a greater illusory experience in the visuotactile-auditory condition compared to visual-auditory condition, since the tactile input was more task-relevant and come from the same perceived spatial location as the visual input, resulting in the tactile input being up-weighted, and therefore had a greater influence on the combined illusory percept, whereas the auditory input was comparatively down-weighted. However, when there is an absence of tactile input, such as in the visual-auditory condition, then the temporal synchrony of the auditory input and visual input serves to upweight the auditory input, allowing a greater influence on the combined percept within the resizing illusion. 

Regarding performance findings, our positive control analyses showed that there was a significant difference between baseline and experimental conditions for left and right dot touch tasks, with participants accurately placing their finger on the dot in the baseline condition for the right dot task, and then touching around a centimetre too close to their own bodies in both experimental conditions due to the perceived elongation of the finger in the experimental conditions. For the left dot touch data, participants were less accurate in their finger placement in the baseline condition, but still placed their finger significantly closer to their own bodies in both experimental conditions, indicating a perceived elongation of their finger in both right and left dot touch tasks. Additionally, in the ruler judgement task, participants reported the tip of their finger to be significantly further away in both experimental conditions compared to the baseline condition. This indicates that they experienced their finger as being longer in both experimental conditions when compared to the baseline non-illusion condition. These findings indicate success of the positive control analyses, showing that these performance tasks can highlight the differences between baseline and experimental conditions. 

Referring to the confirmatory analyses regarding the dot touch data, our findings showed no significant effect of group or condition for the right dot touch task, and there was no effect of group for the left dot touch task, however there was an effect of condition, with participants placing their finger significantly closer to their bodies in the conditions without touch (V/VA) compared to the conditions with tactile input (VT/VTA). This finding of a significant effect of condition for the left dot touch data could be explained as a transference effect of stretching from the manipulated hand (right) to the non-manipulated hand (left). @petkova2011a found whilst using a full body illusion and fMRI, evidence for a spread of ownership across connected body parts. Therefore, the resizing of the right hand could likely spread to the left unmanipulated hand, meaning participants felt as though this hand had also been resized, which is supported by the positive control analyses for the left dot touch task in which we found a significant effect of the illusion in the experimental conditions without manipulation of this hand. It is possible that the tactile inputs in the VT/VTA illusion could provide a grounding effect, wherein the participant’s hand is grounded to the spatial location within the augmented reality system, which does not occur for visual only or visual audio manipulations. This is further supported by @ernst2002a optimal integration model, with the tactile input providing a more accurate location estimate than the visual input alone, as the visual input is less reliable than the tactile input and is therefore down-weighted in comparison to tactile input which is up-weighted within the combined percept. This spatial grounding in the tactile input conditions in conjunction with the transference effects mentioned previously, could explain why we see a significant difference between experimental conditions in the left dot touch task. This is, however, speculative, and further research would be needed to assess the replicability of this effect. Finally, our ruler judgement data also showed no significant effect of condition or group, indicating that the addition of non-naturalistic auditory input showed no facilitatory effects for either performance task.

The rationale for including two performance tasks in the present study came from previous discordance in the literature with @kammers2009a finding an impact on body image, but not body schema, in the rubber hand illusion, whereas @newport2010a found distortions in body schema using the rubber hand illusion and supernumerary limbs. The use of differing measures of body representations in previous literature often results in different findings, and this discordance between body image and body schema is one example of when this occurs regarding body illusions. Here, we see evidence for an impact of resizing illusions on both body image and body schema, as evidenced through the positive control analyses, showing that resizing illusions affect one’s percept of the body (body image) in addition to the control of the body in an external environment (body schema). The rubber hand illusion differs from the resizing illusion used here, in that the present manipulation does not attempt to relocate the hand, but rather attempts to change the representation of the finger to be longer. Therefore, it could be that when changing an existing part of one’s body, both body image and body schema are affected, whilst when attempting to create a new sensation of one’s body in a different location, impact on body schema is dependent on the experimental manipulations being used. Additionally, in the current study we use an augmented reality system that is similar to that used by @newport2010a, and this system could be producing a more vivid illusion than the rubber hand illusion typically creates. 

The increasing pitch tone that was used as the non-naturalistic auditory input in the current study was chosen as it closely reflected that used by @tajadura-jim2017a, who previously found increases in estimations of finger length when accompanied by an increasing pitch tone compared to a decreasing or constant tone. However, in the current experiment, we cannot claim that the effect of an increase in subjective experience of the resizing illusion when this non-naturalistic auditory input is added, is unique to a rising pitch tone. It is possible that other auditory inputs, could elicit similar effects in increasing subjective experience. Examples might include naturalistic inputs, perhaps of the bones in the finger creaking as it is stretched, akin to the auditory inputs heard during chiropractic treatments, or an unrelated auditory input, such as a constant tone during the resizing conditions. It is also possible that the increasing pitch tone that was used in the current study could be manipulated to be presented in steps rather than as a constant tone, to assess whether the same effects of increasing illusory experience are seen in different presentations of a rising pitch tone, or if the addition of any tone at all would increase subjective illusory experience by directing attention towards the illusory manipulation. Nevertheless, the findings from the current study enhance our understanding of the role that auditory input can play in resizing illusions, and further research into the efficacy of alternate auditory inputs should be conducted to consolidate current findings. 

Resizing illusions have been found to reduce subjective pain ratings in participants with chronic pain conditions affecting the hands [@preston2011a], back [@diers2013a], and knees [@stanton2018a]. The findings from the present study serve to enhance our understanding of the conditions under which these manipulations can affect the personal experience of such illusions. Previously, we have demonstrated that around 30% of participants experience effective resizing illusions via a unimodal visual condition [@hansford2023a]. Here, we show that subjective illusion strength during the unimodal visual presentation of finger stretching can be increased through the addition of a simultaneous non-naturalistic auditory input. It is, therefore, possible that when using these resizing illusions for the treatment of chronic pain, it may be beneficial to include non-naturalistic auditory input to increase the subjective illusion strength for patients during the illusion, and consequently, potentially increase attenuation of pain. The unimodal visual condition has been suggested as the most accessible version of resizing illusions [@hansford2023a] as it has the potential to be delivered via a mobile phone application without the need for a researcher to add tactile inputs to the illusion. The incorporation of auditory inputs would not require the presence of a researcher either, and therefore, is a potential method to utilise multisensory integration during unimodal visual applications of these illusions to increase subjective illusion strength, which could in turn increase the analgesic effect of these illusions in a chronic pain sample. Future research should, therefore, assess whether the addition of auditory input has a similar effect in enhancing the strength of these illusions in chronic pain patients, as has been demonstrated here in participants who do not experience chronic pain. 

## 5. Conclusions {.unnumbered}

We found that the addition of non-naturalistic auditory input can increase the subjective illusion strength of resizing illusions, however we found no facilitatory effects of the auditory input for any performance measures of illusion strength. We address the previous discordance in the literature surrounding the impact of hand-based illusions on body image and body schema, showing that in a resizing hand-based illusion, the manipulation affects both representations of the bodily self. Additionally, this study highlights the potential for non-naturalistic auditory input to be included in resizing illusions used to treat chronic pain, whilst inviting further research to assess the impact of non-naturalistic auditory input in chronic pain patient samples. In addition, our findings invite further research into the uniqueness of a rising pitch tone as the presentation of a non-naturalistic auditory input, in order to assess whether this alone causes an increase in subjective illusion strength. Finally, we highlight the differential effects of these resizing illusions on conscious subjective experience versus unconscious performance-based measures; further elucidating the mechanisms by which such manipulations can alter bodily experience. 

## Acknowledgements {.unnumbered}

The authors would like to acknowledge B.P.A. Quinn, for creation of the schematic in Figure 1, and the Pain Relief Foundation studentship and BBSRC grant BB/V007580/1, for funding this work. 

\newpage

## References {.unnumbered} 
<div id='refs'> </div>

\newpage

## Supplementary Materials {.unnumbered}

S1. Video of finger stretching

\vspace{5mm}

A video of a participant undergoing a visual-tactile illusion can be seen at the following OSF link: https://osf.io/ek8cd

\vspace{15mm}

\beginsupplement

S2. Ownership Data

```{r figureS2, fig.cap="Ownership data for participants across all conditions. Each mean rating is above 50 indicating experience of ownership of the hand during the illusion (Baseline Audio: M = 82.4, SD = 30.6, Baseline Non-audio: M = 89.7, SD = 15.1, V/VA Audio: M = 58.4, SD = 31.3, V/VA Non-Audio: M = 52.3, SD = 27.9, VT/VTA Audio: M = 64.9, SD = 30.8, VT/VTA Non-Audio: M = 73.4, SD = 25.8).", fig.align='center', out.width="50%", echo=FALSE}

knitr::include_graphics('figures/Illusion2.pdf')

```

\newpage

S3. Disownership Data

```{r figureS3, fig.cap="Disownership average data for participants across all conditions. Each mean rating is below 50 indicating experience of no experiences of disownership of the hand during the illusion (Baseline Audio: M = 5.21, SD = 8.93, Baseline Non-audio: M = 8.33, SD = 13.6, V/VA Audio: M = 38.4, 29.0, V/VA Non-Audio: M = 40.5, 32.0, VT/VTA Audio: M = 26.4, SD = 25.6, VT/VTA Non-Audio: M = 18.2, 24.6).", fig.align='center', out.width="50%", echo=FALSE}

knitr::include_graphics('figures/disownershipaverage.pdf')

```

S4. Control Data

```{r figureS4, fig.cap="Control average data for participants across all conditions. Each mean rating is below 50 indicating no violation of control statements during the illusion (Baseline Audio: M= 4.12, SD = 8.55, Baseline Non-audio: M = 4.09, SD = 6.76, V/VA Audio: M = 10.5, SD = 13.2, V/VA Non-Audio: M = 7.91, SD = 16.0, VT/VTA Audio: M = 8.81, SD = 11.2, VT/VTA Non-Audio: M = 3.17, SD = 5.49).", fig.align='center', out.width="50%", echo=FALSE}

knitr::include_graphics('figures/controlaverage.pdf')

```

\newpage

S5. Positive Control Right Dot Touch

```{r figureS5, fig.cap="Dot Touch Data in centimetres for Baseline, V/VA, and VT/VTA conditions for right hand data. Group is indicated by colour, with red showing the no audio group and blue showing the audio group.", fig.align='center', out.width="50%", echo=FALSE}

knitr::include_graphics('figures/PCrightdot.pdf')

```


S6. Positive Control left Dot Touch

```{r figureS6, fig.cap="Dot Touch Data in centimetres for Baseline, V/VA, and VT/VTA conditions for left hand data. Group is indicated by colour, with red showing the no audio group and blue showing the audio group.", fig.align='center', out.width="50%", echo=FALSE}

knitr::include_graphics('figures/PCleftdot.pdf')

```

\newpage

S7. Positive Control Ruler Judgement

```{r figureS7, fig.cap="Ruler Judgement data in centimetres for baseline, V/VA, and VT/VTA conditions. Group is indicated by colour, with red showing the no audio group and blue showing the audio group.", fig.align='center', out.width="50%", echo=FALSE}

knitr::include_graphics('figures/PCruler.pdf')

```
